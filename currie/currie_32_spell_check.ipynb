{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# referred https://towardsdatascience.com/creating-a-spell-checker-with-tensorflow-d35b23939f60\n",
    "# https://github.com/Currie32/Spell-Checker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import namedtuple\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import time\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30423 words in Alices_Adventures_in_Wonderland_by_Lewis_Carroll.rtf.\n",
      "There are 361612 words in Anna_Karenina_by_Leo_Tolstoy.rtf.\n",
      "There are 113452 words in David_Copperfield_by_Charles_Dickens.rtf.\n",
      "There are 433993 words in Don_Quixote_by_Miguel_de_Cervantes.rtf.\n",
      "There are 166996 words in Dracula_by_Bram_Stoker.rtf.\n",
      "There are 163109 words in Emma_by_Jane_Austen.rtf.\n",
      "There are 78912 words in Frankenstein_by_Mary_Shelley.rtf.\n",
      "There are 191598 words in Great_Expectations_by_Charles_Dickens.rtf.\n",
      "There are 105428 words in Grimms_Fairy_Tales_by_The_Brothers_Grimm.rtf.\n",
      "There are 25395 words in Metamorphosis_by_Franz_Kafka.rtf.\n",
      "There are 165188 words in Oliver_Twist_by_Charles_Dickens.rtf.\n",
      "There are 126999 words in Pride_and_Prejudice_by_Jane_Austen.rtf.\n",
      "There are 110213 words in The_Adventures_of_Sherlock_Holmes_by_Arthur_Conan_Doyle.rtf.\n",
      "There are 96185 words in The_Adventures_of_Tom_Sawyer_by_Mark_Twain.rtf.\n",
      "There are 480495 words in The_Count_of_Monte_Cristo_by_Alexandre_Dumas.rtf.\n",
      "There are 83657 words in The_Picture_of_Dorian_Gray_by_Oscar_Wilde.rtf.\n",
      "There are 53211 words in The_Prince_by_Nicolo_Machiavelli.rtf.\n",
      "There are 194282 words in The_Romance_of_Lust_by_Anonymous.rtf.\n",
      "There are 9463 words in The_Yellow_Wallpaper_by_Charlotte_Perkins_Gilman.rtf.\n",
      "There are 33464 words in Through_the_Looking_Glass_by_Lewis_Carroll.rtf.\n",
      "Total number of words are 3024075.\n"
     ]
    }
   ],
   "source": [
    "def load_book(path):\n",
    "    \"\"\"Load a book from its file\"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        book = f.read()\n",
    "    return book\n",
    "\n",
    "path = './books/'\n",
    "book_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "book_files = book_files[1:]\n",
    "\n",
    "# Load the books using the file names\n",
    "books = []\n",
    "for book in book_files:\n",
    "    books.append(load_book(path+book))\n",
    "\n",
    "# Compare the number of words in each book \n",
    "tot_words=0\n",
    "for i in range(len(books)):\n",
    "    num_words = len(books[i].split())\n",
    "    tot_words+=num_words\n",
    "    print(\"There are {} words in {}.\".format(num_words, book_files[i]))\n",
    "print(\"Total number of words are {}.\".format(tot_words))\n",
    "# print books[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Remove unwanted characters and extra spaces from the text'''\n",
    "    text = re.sub(r'\\n', ' ', text) \n",
    "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n",
    "    text = re.sub('a0','', text)\n",
    "    text = re.sub('\\'92t','\\'t', text)\n",
    "    text = re.sub('\\'92s','\\'s', text)\n",
    "    text = re.sub('\\'92m','\\'m', text)\n",
    "    text = re.sub('\\'92ll','\\'ll', text)\n",
    "    text = re.sub('\\'91','', text)\n",
    "    text = re.sub('\\'92','', text)\n",
    "    text = re.sub('\\'93','', text)\n",
    "    text = re.sub('\\'94','', text)\n",
    "    text = re.sub('\\.','. ', text)\n",
    "    text = re.sub('\\!','! ', text)\n",
    "    text = re.sub('\\?','? ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return text\n",
    "\n",
    "# Clean the text of the books\n",
    "clean_books = []\n",
    "for book in books:\n",
    "    clean_books.append(clean_text(book))\n",
    "\n",
    "# print clean_books[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 78 characters.\n",
      "[' ', '!', '\"', '$', '&', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<EOS>', '<GO>', '<PAD>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to convert the vocabulary (characters) to integers\n",
    "vocab_to_int = {}\n",
    "count = 0\n",
    "for book in clean_books:\n",
    "    for character in book:\n",
    "        if character not in vocab_to_int:\n",
    "            vocab_to_int[character] = count\n",
    "            count += 1\n",
    "\n",
    "# Add special tokens to vocab_to_int\n",
    "codes = ['<PAD>','<EOS>','<GO>']\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = count\n",
    "    count += 1\n",
    "\n",
    "# Check the size of vocabulary and all of the values\n",
    "vocab_size = len(vocab_to_int)\n",
    "print(\"The vocabulary contains {} characters.\".format(vocab_size))\n",
    "print(sorted(vocab_to_int))\n",
    "\n",
    "# Create another dictionary to convert integers to their respective characters\n",
    "int_to_vocab = {}\n",
    "for character, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133494 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Split the text from the books into sentences.\n",
    "sentences = []\n",
    "for book in clean_books:\n",
    "    for sentence in book.split('. '):\n",
    "        sentences.append(sentence + '.')\n",
    "print(\"There are {} sentences.\".format(len(sentences)))\n",
    "\n",
    "# print sentences[:5]\n",
    "# Note that 1st sentence won't effect because it is long and will be filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert sentences to integers\n",
    "int_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    int_sentence = []\n",
    "    for character in sentence:\n",
    "        int_sentence.append(vocab_to_int[character])\n",
    "    int_sentences.append(int_sentence)\n",
    "\n",
    "# Find the length of each sentence\n",
    "lengths = []\n",
    "for sentence in int_sentences:\n",
    "    lengths.append(len(sentence))\n",
    "lengths = pd.DataFrame(lengths, columns=[\"counts\"])\n",
    "\n",
    "#print lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use 59885 to train and test our model.\n",
      "('Number of training sentences:', 50902)\n",
      "('Number of testing sentences:', 8983)\n"
     ]
    }
   ],
   "source": [
    "# Limit the data we will use to train our model\n",
    "max_length = 92\n",
    "min_length = 10\n",
    "\n",
    "good_sentences = []\n",
    "\n",
    "for sentence in int_sentences:\n",
    "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
    "        good_sentences.append(sentence)\n",
    "\n",
    "print(\"We will use {} to train and test our model.\".format(len(good_sentences)))\n",
    "\n",
    "# Split the data into training and testing sentences\n",
    "training, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 2)\n",
    "\n",
    "print(\"Number of training sentences:\", len(training))\n",
    "print(\"Number of testing sentences:\", len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the sentences by length to reduce padding, which will allow the model to train faster\n",
    "training_sorted = []\n",
    "testing_sorted = []\n",
    "\n",
    "for i in range(min_length, max_length+1):\n",
    "    for sentence in training:\n",
    "        if len(sentence) == i:\n",
    "            training_sorted.append(sentence)\n",
    "    for sentence in testing:\n",
    "        if len(sentence) == i:\n",
    "            testing_sorted.append(sentence)\n",
    "\n",
    "# Check to ensure the sentences have been selected and sorted correctly\n",
    "#for i in range(5):\n",
    "#    print(training_sorted[i], len(training_sorted[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    #Returns list of numbers.\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0,1,1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0,1,1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i+1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(vocab_to_int[random_letter])\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass     \n",
    "        i += 1\n",
    "    return noisy_sentence\n",
    "\n",
    "# Check to ensure noise_maker is making mistakes correctly.\n",
    "#threshold = 0.9\n",
    "#for sentence in training_sorted[:5]:\n",
    "#    print(sentence)\n",
    "#    print(noise_maker(sentence, threshold)) #Returns list of numbers\n",
    "#    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n",
    "    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n",
    "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
    "\n",
    "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length\n",
    "\n",
    "def process_encoding_input(targets, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    with tf.name_scope(\"process_encoding\"):\n",
    "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input\n",
    "\n",
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    if direction == 1:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "\n",
    "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                         input_keep_prob = keep_prob)\n",
    "\n",
    "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n",
    "                                                              rnn_inputs,\n",
    "                                                              sequence_length,\n",
    "                                                              dtype=tf.float32)\n",
    "\n",
    "            return enc_output, enc_state\n",
    "        \n",
    "        \n",
    "    if direction == 2:\n",
    "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
    "            for layer in range(num_layers):\n",
    "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                            input_keep_prob = keep_prob)\n",
    "\n",
    "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                            input_keep_prob = keep_prob)\n",
    "\n",
    "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                            cell_bw, \n",
    "                                                                            rnn_inputs,\n",
    "                                                                            sequence_length,\n",
    "                                                                            dtype=tf.float32)\n",
    "            # Join outputs since we are using a bidirectional RNN\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            # Use only the forward state because the model can't use both states at once\n",
    "            return enc_output, enc_state[0]\n",
    "        \n",
    "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_target_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    with tf.name_scope(\"Training_Decoder\"):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=targets_length,\n",
    "                                                            time_major=False)\n",
    "\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state,\n",
    "                                                           output_layer) \n",
    "\n",
    "        training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                               output_time_major=False,\n",
    "                                                               impute_finished=True,\n",
    "                                                               maximum_iterations=max_target_length)\n",
    "        return training_logits\n",
    "\n",
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_target_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    with tf.name_scope(\"Inference_Decoder\"):\n",
    "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                    start_tokens,\n",
    "                                                                    end_token)\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                            inference_helper,\n",
    "                                                            initial_state,\n",
    "                                                            output_layer)\n",
    "\n",
    "        inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                                output_time_major=False,\n",
    "                                                                impute_finished=True,\n",
    "                                                                maximum_iterations=max_target_length)\n",
    "\n",
    "        return inference_logits\n",
    "\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
    "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
    "        for layer in range(num_layers):\n",
    "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                         input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  inputs_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "    \n",
    "    with tf.name_scope(\"Attention_Wrapper\"):\n",
    "        dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
    "        #dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
    "    \n",
    "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state,_zero_state_tensors(rnn_size, batch_size, tf.float32))\n",
    "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state,_zero_state_tensors(rnn_size, batch_size, tf.float32))\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, \n",
    "                                                  targets_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_target_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_target_length,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return training_logits, inference_logits\n",
    "\n",
    "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n",
    "                                           enc_embed_input, keep_prob, direction)\n",
    "    \n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        dec_embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        inputs_length, \n",
    "                                                        targets_length, \n",
    "                                                        max_target_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers,\n",
    "                                                        direction)\n",
    "    \n",
    "    return training_logits, inference_logits\n",
    "\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "def get_batches(sentences, batch_size, threshold):\n",
    "    \"\"\"Batch sentences, noisy sentences, and the lengths of their sentences together.\n",
    "       With each epoch, sentences will receive new mistakes\"\"\"\n",
    "    \n",
    "    for batch_i in range(0, len(sentences)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
    "        \n",
    "        sentences_batch_noisy = []\n",
    "        for sentence in sentences_batch:\n",
    "            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n",
    "            \n",
    "        sentences_batch_eos = []\n",
    "        for sentence in sentences_batch:\n",
    "            sentence.append(vocab_to_int['<EOS>'])\n",
    "            sentences_batch_eos.append(sentence)\n",
    "            \n",
    "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
    "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_sentences_lengths = []\n",
    "        for sentence in pad_sentences_batch:\n",
    "            pad_sentences_lengths.append(len(sentence))\n",
    "        \n",
    "        pad_sentences_noisy_lengths = []\n",
    "        for sentence in pad_sentences_noisy_batch:\n",
    "            pad_sentences_noisy_lengths.append(len(sentence))\n",
    "        \n",
    "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The default parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "num_layers = 2\n",
    "rnn_size = 512\n",
    "embedding_size = 128\n",
    "learning_rate = 0.0005\n",
    "direction = 2\n",
    "threshold = 0.95\n",
    "keep_probability = 0.75\n",
    "\n",
    "# v1: Default from https://github.com/Currie32/Spell-Checker/blob/master/SpellChecker.ipynb\n",
    "# v2: Adding num_layers as list [2,4]. Changed stop to 4 (consequtively we need 4 now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      inputs_length,\n",
    "                                                      targets_length,\n",
    "                                                      max_target_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size,\n",
    "                                                      embedding_size,\n",
    "                                                      direction)\n",
    "\n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "\n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
    "    \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n",
    "                                                targets, \n",
    "                                                masks)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    with tf.name_scope(\"optimze\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "    # Merge all of the summaries\n",
    "    merged = tf.summary.merge_all()    \n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n",
    "                    'predictions', 'merged', 'train_op','optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, log_string):\n",
    "    '''Train the RNN'''\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Used to determine when to stop the training early\n",
    "        testing_loss_summary = []\n",
    "\n",
    "        # Keep track of which batch iteration is being trained\n",
    "        iteration = 0\n",
    "        \n",
    "        display_step = 30 # The progress of the training will be displayed after every 30 batches\n",
    "        stop_early = 0 \n",
    "        stop = 4 # If the batch_loss_testing does not decrease in 3 consecutive checks, stop training\n",
    "        per_epoch = 3 # Test the model 3 times per epoch\n",
    "        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n",
    "\n",
    "        print()\n",
    "        print(\"Training Model: {}\".format(log_string))\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./logs/ori_v2/train/{}'.format(log_string), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/ori_v2/test/{}'.format(log_string))\n",
    "\n",
    "        for epoch_i in range(1, epochs+1): \n",
    "            batch_loss = 0\n",
    "            batch_time = 0\n",
    "            \n",
    "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
    "                    get_batches(training_sorted, batch_size, threshold)):\n",
    "                start_time = time.time()\n",
    "\n",
    "                summary, loss, _ = sess.run([model.merged,\n",
    "                                             model.cost, \n",
    "                                             model.train_op], \n",
    "                                             {model.inputs: input_batch,\n",
    "                                              model.targets: target_batch,\n",
    "                                              model.inputs_length: input_length,\n",
    "                                              model.targets_length: target_length,\n",
    "                                              model.keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "                batch_loss += loss\n",
    "                end_time = time.time()\n",
    "                batch_time += end_time - start_time\n",
    "\n",
    "                # Record the progress of training\n",
    "                train_writer.add_summary(summary, iteration)\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "                if batch_i % display_step == 0 and batch_i > 0:\n",
    "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(epoch_i,\n",
    "                                  epochs, \n",
    "                                  batch_i, \n",
    "                                  len(training_sorted) // batch_size, \n",
    "                                  batch_loss / display_step, \n",
    "                                  batch_time))\n",
    "                    batch_loss = 0\n",
    "                    batch_time = 0\n",
    "\n",
    "                #### Testing ####\n",
    "                if batch_i % testing_check == 0 and batch_i > 0:\n",
    "                    batch_loss_testing = 0\n",
    "                    batch_time_testing = 0\n",
    "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
    "                            get_batches(testing_sorted, batch_size, threshold)):\n",
    "                        start_time_testing = time.time()\n",
    "                        summary, loss = sess.run([model.merged,\n",
    "                                                  model.cost], \n",
    "                                                     {model.inputs: input_batch,\n",
    "                                                      model.targets: target_batch,\n",
    "                                                      model.inputs_length: input_length,\n",
    "                                                      model.targets_length: target_length,\n",
    "                                                      model.keep_prob: 1})\n",
    "\n",
    "                        batch_loss_testing += loss\n",
    "                        end_time_testing = time.time()\n",
    "                        batch_time_testing += end_time_testing - start_time_testing\n",
    "\n",
    "                        # Record the progress of testing\n",
    "                        test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                    n_batches_testing = batch_i + 1\n",
    "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                          .format(batch_loss_testing / n_batches_testing, \n",
    "                                  batch_time_testing))\n",
    "                    \n",
    "                    batch_time_testing = 0\n",
    "\n",
    "                    # If the batch_loss_testing is at a new minimum, save the model\n",
    "                    testing_loss_summary.append(batch_loss_testing)\n",
    "                    if batch_loss_testing <= min(testing_loss_summary):\n",
    "                        print('New Record!') \n",
    "                        stop_early = 0\n",
    "                        checkpoint = \"./{}.ckpt\".format(log_string)\n",
    "                        saver = tf.train.Saver()\n",
    "                        saver.save(sess, checkpoint)\n",
    "\n",
    "                    else:\n",
    "                        print(\"No Improvement.\")\n",
    "                        stop_early += 1\n",
    "                        if stop_early == stop:\n",
    "                            break\n",
    "\n",
    "            if stop_early == stop:\n",
    "                print(\"Stopping Training.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## DONT RUN! THIS IS FOR TRAINING!\n",
    "\n",
    "# # Train the model with the desired tuning parameters\n",
    "\n",
    "# for keep_probability in [0.75]:\n",
    "#     for num_layers in [2,4]:\n",
    "#         for threshold in [0.95]:\n",
    "#             log_string = 'kp={},nl={},th={}_v2'.format(keep_probability,\n",
    "#                                                     num_layers,\n",
    "#                                                     threshold) \n",
    "#             model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n",
    "#                                 learning_rate, embedding_size, direction)\n",
    "#             train(model, epochs, log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building graph\n",
      "128\n",
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95_v2.ckpt\n",
      "\n",
      "Text\n",
      "  Word Ids:    [57, 9, 23, 20, 20, 7, 5, 19, 7, 6, 19, 22, 7, 2, 2, 7, 8, 16, 20, 1, 38, 19, 28, 24, 8, 24, 19, 7, 6, 19, 28, 39, 24, 19, 39, 13, 16, 19, 5, 23, 23, 22, 19, 1, 13, 19, 6, 1, 16, 22, 39, 19, 23, 27, 23, 0, 39, 22, 4, 39, 43, 19]\n",
      "  Input Words: Spellin is difficult, whch is wyh you need to study everyday. \n",
      "\n",
      "Summary\n",
      "  Word Ids:       [57, 9, 23, 20, 20, 7, 5, 19, 7, 6, 19, 22, 7, 2, 2, 7, 8, 16, 20, 1, 38, 19, 28, 24, 24, 19, 7, 6, 19, 28, 39, 19, 39, 13, 16, 19, 5, 23, 23, 22, 19, 1, 13, 19, 6, 1, 16, 22, 39, 19, 23, 27, 23, 0, 39, 19, 1, 13, 19, 6, 1, 16, 22]\n",
      "  Response Words: Spellin is difficult, whh is wy you need to study every to stud\n",
      "128\n",
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95_v2.ckpt\n",
      "\n",
      "Text\n",
      "  Word Ids:    [41, 24, 23, 19, 2, 7, 0, 6, 1, 19, 22, 4, 39, 6, 19, 13, 2, 19, 24, 23, 0, 19, 23, 31, 7, 6, 1, 23, 5, 8, 23, 19, 7, 5, 19, 1, 24, 19, 8, 13, 16, 5, 1, 0, 39, 19, 28, 23, 0, 23, 19, 27, 0, 23, 39, 19, 24, 4, 0, 22, 19, 2, 13, 0, 19, 47, 13, 20, 20, 39, 43, 19]\n",
      "  Input Words: The first days of her existence in th country were vrey hard for Dolly. \n",
      "\n",
      "Summary\n",
      "  Word Ids:       [41, 24, 23, 19, 2, 7, 0, 6, 1, 19, 22, 4, 39, 6, 19, 13, 2, 19, 24, 23, 0, 19, 23, 31, 7, 6, 1, 23, 5, 8, 23, 19, 7, 5, 19, 1, 24, 19, 8, 13, 16, 5, 1, 0, 39, 19, 28, 23, 0, 23, 19, 27, 23, 0, 23, 19, 27, 23, 0, 23, 19, 27, 23, 0, 23, 19, 27, 23, 0, 23, 19, 27, 23]\n",
      "  Response Words: The first days of her existence in th country were vere vere vere vere ve\n",
      "128\n",
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95_v2.ckpt\n",
      "\n",
      "Text\n",
      "  Word Ids:    [41, 24, 7, 19, 7, 6, 19, 0, 23, 4, 20, 20, 39, 19, 6, 13, 21, 23, 1, 24, 7, 5, 10, 19, 7, 21, 9, 0, 23, 6, 6, 7, 27, 19, 1, 24, 4, 4, 1, 19, 28, 23, 19, 6, 24, 13, 16, 20, 22, 19, 20, 13, 13, 30, 19, 7, 5, 1, 13, 19, 0, 7, 10, 1, 19, 4, 28, 4, 39, 65, 19]\n",
      "  Input Words: Thi is really something impressiv thaat we should look into rigt away! \n",
      "\n",
      "Summary\n",
      "  Word Ids:       [41, 24, 7, 6, 19, 7, 6, 19, 0, 23, 4, 20, 20, 39, 19, 6, 13, 21, 23, 1, 24, 7, 5, 10, 19, 7, 21, 9, 0, 23, 6, 6, 7, 27, 23, 19, 7, 21, 9, 0, 23, 6, 6, 7, 27, 23, 19, 7, 21, 9, 0, 23, 6, 6, 7, 27, 23, 19, 1, 24, 4, 1, 19, 28, 23, 19, 6, 24, 13, 16, 20, 22]\n",
      "  Response Words: This is really something impressive impressive impressive that we should\n"
     ]
    }
   ],
   "source": [
    "def text_to_ints(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int[word] for word in text]\n",
    "\n",
    "#checkpoint = \"./kp=0.75,nl=2,th=0.95_v1.ckpt\" #Version 1\n",
    "checkpoint = \"./kp=0.75,nl=2,th=0.95_v2.ckpt\" #Version 2\n",
    "\n",
    "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction)\n",
    "print(\"Done building graph\")\n",
    "# Create your own sentence or use one from the dataset\n",
    "texts = [\"Spellin is difficult, whch is wyh you need to study everyday.\",\n",
    "         \"The first days of her existence in th country were vrey hard for Dolly.\",\n",
    "         \"Thi is really something impressiv thaat we should look into rigt away!\"]\n",
    "for text in texts:\n",
    "    text = text_to_ints(text)\n",
    "\n",
    "    #random = np.random.randint(0,len(testing_sorted))\n",
    "    #text = testing_sorted[random]\n",
    "    #text = noise_maker(text, 0.95)\n",
    "\n",
    "     \n",
    "    print (batch_size)    \n",
    "    with tf.Session() as sess:\n",
    "        # Load saved model\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "        #Multiply by batch_size to match the model's input parameters\n",
    "        answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
    "                                                     model.inputs_length: [len(text)]*batch_size,\n",
    "                                                     model.targets_length: [len(text)+1], \n",
    "                                                     model.keep_prob: [1.0]})[0]\n",
    "\n",
    "    # Remove the padding from the generated sentence\n",
    "    pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "    print('\\nText')\n",
    "    print('  Word Ids:    {}'.format([i for i in text]))\n",
    "    print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "    print('\\nSummary')\n",
    "    print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "    print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95_v2.ckpt\n",
      "[58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43]\n",
      "[[58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43], [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43], [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43], [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43], [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43], [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43], [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43], [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43], [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43], [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43]]\n",
      "\n",
      "Text\n",
      "  Word Ids:    [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43]\n",
      "  Input Words: Harper f1 uc0u65533 f0 s.\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [58, 4, 0, 9, 23, 0, 19, 2, 3, 19, 16, 8, 15, 16, 55, 12, 12, 63, 63, 19, 2, 15, 19, 6, 43, 76]\n",
      "  Response Words: Harper f1 uc0u65533 f0 s.<EOS>\n",
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95_v2.ckpt\n",
      "[57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43]\n",
      "[[57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43], [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43], [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43], [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43], [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43], [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43], [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43], [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43], [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43], [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43]]\n",
      "\n",
      "Text\n",
      "  Word Ids:    [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 2, 19, 20, 13, 0, 24, 43]\n",
      "  Input Words: Someting begant o trickle on thef lorh.\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [57, 13, 21, 23, 1, 7, 5, 10, 19, 17, 23, 10, 4, 5, 1, 19, 13, 19, 1, 0, 7, 8, 30, 20, 23, 19, 13, 5, 19, 1, 24, 23, 19, 20, 13, 0, 24, 43, 76]\n",
      "  Response Words: Someting begant o trickle on the lorh.<EOS>\n",
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95_v2.ckpt\n",
      "[35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43]\n",
      "[[35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43], [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43], [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43], [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43], [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43], [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43], [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43], [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43], [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43], [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43]]\n",
      "\n",
      "Text\n",
      "  Word Ids:    [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 33, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43]\n",
      "  Input Words: 'Oliver,' said Harry Maylie, qin a low voice, 'et me speak a worjd with yuo.\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [35, 54, 20, 7, 27, 23, 0, 38, 35, 19, 6, 4, 7, 22, 19, 58, 4, 0, 0, 39, 19, 51, 4, 39, 20, 7, 23, 38, 19, 66, 7, 5, 19, 4, 19, 20, 13, 28, 19, 27, 13, 7, 8, 23, 38, 19, 35, 23, 1, 19, 21, 23, 19, 6, 9, 23, 4, 30, 19, 4, 19, 28, 13, 0, 22, 19, 28, 7, 1, 24, 19, 39, 16, 13, 43, 76]\n",
      "  Response Words: 'Oliver,' said Harry Maylie, qin a low voice, 'et me speak a word with yuo.<EOS>\n",
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95_v2.ckpt\n",
      "[36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43]\n",
      "[[36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43], [36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43], [36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43], [36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43], [36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43], [36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43], [36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43], [36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43], [36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43], [36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text\n",
      "  Word Ids:    [36, 5, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43]\n",
      "  Input Words: Anod crtsey while youre thinking wat to-n-what to purr.\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [36, 5, 22, 13, 22, 19, 8, 0, 1, 6, 23, 39, 19, 28, 24, 7, 20, 23, 19, 39, 13, 16, 0, 23, 19, 1, 24, 7, 5, 30, 7, 5, 10, 19, 28, 4, 1, 19, 1, 13, 45, 5, 45, 28, 24, 4, 1, 19, 1, 13, 19, 9, 16, 0, 0, 43]\n",
      "  Response Words: Andod crtsey while youre thinking wat to-n-what to purr.\n",
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95_v2.ckpt\n",
      "[59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43]\n",
      "[[59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43], [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43], [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43], [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43], [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43], [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43], [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43], [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43], [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43], [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43]]\n",
      "\n",
      "Text\n",
      "  Word Ids:    [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 25, 19, 58, 36, 32, 41, 49, 50, 19, 70, 59, 62, 19, 51, 0, 6, 43]\n",
      "  Input Words: I mentionc no names; buth appy the mna who changes Emma for Harrie!tC HAPTER XIV Mrs.\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [59, 19, 21, 23, 5, 1, 7, 13, 5, 8, 19, 5, 13, 19, 5, 4, 21, 23, 6, 26, 19, 17, 16, 1, 24, 19, 4, 9, 9, 39, 19, 1, 24, 23, 19, 21, 5, 4, 19, 28, 24, 13, 19, 8, 24, 4, 5, 10, 23, 6, 19, 49, 21, 21, 4, 19, 2, 13, 0, 19, 58, 4, 0, 0, 7, 23, 65, 1, 19, 58, 36, 32, 41, 41, 49, 50, 49, 50, 19, 59, 62, 62, 0, 7, 23, 65]\n",
      "  Response Words: I mentionc no names; buth appy the mna who changes Emma for Harrie!t HAPTTERER IVVrie!\n",
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95_v2.ckpt\n",
      "[47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43]\n",
      "[[47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43], [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43], [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43], [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43], [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43], [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43], [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43], [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43], [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43], [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43]]\n",
      "\n",
      "Text\n",
      "  Word Ids:    [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43]\n",
      "  Input Words: Don't frow ike that.\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [47, 13, 5, 35, 1, 19, 2, 0, 13, 28, 19, 7, 30, 23, 19, 1, 24, 4, 1, 43, 76]\n",
      "  Response Words: Don't frow ike that.<EOS>\n",
      "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95_v2.ckpt\n",
      "[32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43]\n",
      "[[32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43], [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43], [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43], [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43], [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43], [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43], [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43], [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43], [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43], [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text\n",
      "  Word Ids:    [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22, 30, 23, 22, 19, 0, 13, 16, 8, 5, 43]\n",
      "  Input Words: Petritsky jumped up suddenly onto his knees and loodked roucn.\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [32, 23, 1, 0, 7, 1, 6, 30, 39, 19, 33, 16, 21, 9, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 23, 22, 19, 16, 9, 19, 6, 16, 22, 22, 23, 5, 20, 39, 19, 13, 5, 1, 13, 19, 24, 7, 6, 19, 30, 5, 23, 23, 6, 19, 4, 5, 22, 19, 20, 13, 13, 22]\n",
      "  Response Words: Petritsky jumped up suddeed up suddenly onto his knees and lood\n"
     ]
    }
   ],
   "source": [
    "for _ in range(7):\n",
    "    #text = text_to_ints(text)\n",
    "\n",
    "    random = np.random.randint(0,len(testing_sorted))\n",
    "    text = testing_sorted[random]\n",
    "    text = noise_maker(text, 0.95)\n",
    "\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "        # Load saved model\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "        print(text)\n",
    "        print([text]*10)\n",
    "        #Multiply by batch_size to match the model's input parameters\n",
    "        answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
    "                                                     model.inputs_length: [len(text)]*batch_size,\n",
    "                                                     model.targets_length: [len(text)+1], \n",
    "                                                     model.keep_prob: [1.0]})[0]\n",
    "\n",
    "    # Remove the padding from the generated sentence\n",
    "    pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "    print('\\nText')\n",
    "    print('  Word Ids:    {}'.format([i for i in text]))\n",
    "    print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "    print('\\nSummary')\n",
    "    print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "    print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_ints(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int[word] for word in text]\n",
    "\n",
    "def run_curie(data,data_type,checkpoint):\n",
    "\n",
    "    pred=[]\n",
    "    print(len(data))\n",
    "#     checkpoint = \"./kp=0.75,nl=2,th=0.95_v2.ckpt\" #Version 2\n",
    "    print(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction)\n",
    "    model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction)\n",
    "    print(\"Done building graph\")\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Load saved model\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "       \n",
    "        for i,sent in enumerate(data):\n",
    "            if i%100==0:\n",
    "                print(i)\n",
    "            text = text_to_ints(sent)           \n",
    "            #Multiply by batch_size to match the model's input parameters\n",
    "            answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
    "                                                     model.inputs_length: [len(text)]*batch_size,\n",
    "                                                     model.targets_length: [len(text)+1], \n",
    "                                                     model.keep_prob: [1.0]})[0]\n",
    "            \n",
    "            # Remove the padding from the generated sentence\n",
    "            pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "            op=\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
    "            pred.append(op)\n",
    "                        \n",
    "#             print(op)\n",
    "        \n",
    "        with open('./currie_' + data_type +checkpoint[2:] + '.pkl', 'wb') as handle:\n",
    "            pkl.dump(pred, handle, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "        print('Saved predictions in pickle')\n",
    "        \n",
    "    return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./JDBv1.0/train.txt', 6000, Counter({1: 5548, 2: 434, 3: 15, 4: 3}))\n",
      "('./JDBv1.0/test-split1.txt', 1711, Counter({1: 1581, 2: 126, 3: 4}))\n",
      "('./JDBv1.0/test-split2.txt', 1711, Counter({1: 1567, 2: 137, 3: 7}))\n",
      "('./JDBv1.0/test-split3.txt', 1711, Counter({1: 1574, 2: 131, 3: 5, 4: 1}))\n",
      "('Train:', 6000)\n",
      "('Test:', 5133)\n",
      "('./Speller Challenge TREC Dataset/Speller Challenge TREC Dataset.txt', 5892, Counter({1: 5030, 2: 824, 3: 35, 4: 3}))\n",
      "('./corpus-webis-qspell-17.csv', 54772, Counter({1: 51506, 2: 2523, 3: 548, 4: 132, 5: 35, 6: 24, 7: 4}))\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import pickle as pkl\n",
    "from eval_all import *\n",
    "# dir(eval_all)\n",
    "jdb_X,jdb_Y,jdb_tX,jdb_tY=read_jdb()\n",
    "trec_X,trec_Y=read_trec()\n",
    "qspell_X,qspell_Y=read_qspell()\n",
    "\n",
    "\n",
    "# checkpoint = \"./kp=0.75,nl=2,th=0.95_v2.ckpt\" #Version 2\n",
    "checkpoint='./kp=0.75,nl=2,th=0.95_ori_v3.ckpt'\n",
    "# checkpoint='./version3_kp=0.75,nl=2,th=0.95.ckpt'\n",
    "# checkpoint='./version4_kp=0.75,nl=2,th=0.95.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "# run_curie(jdb_X,'jdb_train',checkpoint)\n",
    "# run_curie(jdb_tX,'jdb',checkpoint)\n",
    "# run_curie(trec_X,'trec',checkpoint)\n",
    "# run_curie(qspell_X,'qspell',checkpoint)\n",
    "# eval_file(jdb_X,jdb_Y,'currie_jdbkp=0.75,nl=2,th=0.95_v2.ckpt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "evaluating\n",
      "NEW eval\n",
      "(6744.1, ' out of ', 54772, ' not in the input')\n",
      "(59.1, ' out of ', 6744.1, ' correct\\n')\n",
      "('Method X : ', (0.00876321525481532, 0.8779504126195866, 0.8768713941429928))\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from eval_all import *\n",
    "\n",
    "\n",
    "# eval_file(jdb_X,jdb_Y,'currie_jdb_trainkp=0.75,nl=2,th=0.95_ori_v3.ckpt.pkl')\n",
    "# eval_file(jdb_tX,jdb_tY,'currie_jdbkp=0.75,nl=2,th=0.95_ori_v3.ckpt.pkl')\n",
    "# eval_file(trec_X,trec_Y,'currie_treckp=0.75,nl=2,th=0.95_ori_v3.ckpt.pkl')\n",
    "eval_file(qspell_X,qspell_Y,'currie_qspellkp=0.75,nl=2,th=0.95_ori_v3.ckpt.pkl')\n",
    "\n",
    "# eval_file(jdb_X,jdb_Y,'currie_jdbkp=0.75,nl=2,th=0.95_v2.ckpt.pkl')\n",
    "# eval_file(trec_X,trec_Y,'currie_treckp=0.75,nl=2,th=0.95_v2.ckpt.pkl')\n",
    "# eval_file(qspell_X,qspell_Y,'currie_qspellkp=0.75,nl=2,th=0.95_v2.ckpt.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
